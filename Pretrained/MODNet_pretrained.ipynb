{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODNet on the PBE, HSE and experimental datasets successively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we will see if training first the MODNet model on the PBE, HSE and experimental datasets and then, fine tuning the model on both the HSE and experimental datasets, and finally, fine tuning the model on the experimental dataset will improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:07:29.051443Z",
     "start_time": "2021-06-14T16:07:29.030412Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_threading():\n",
    "    import os\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "    os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:07:30.466650Z",
     "start_time": "2021-06-14T16:07:30.462310Z"
    }
   },
   "outputs": [],
   "source": [
    "setup_threading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:07:50.146887Z",
     "start_time": "2021-06-14T16:07:31.566174Z"
    }
   },
   "outputs": [],
   "source": [
    "from modnet.preprocessing import MODData\n",
    "from modnet.models.vanilla import MODNetModel\n",
    "from modnet.hyper_opt.fit_genetic import FitGenetic\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:07:50.167068Z",
     "start_time": "2021-06-14T16:07:50.149320Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from modnet.preprocessing import MODData\n",
    "\n",
    "def shuffle_MD(data,random_state=10):\n",
    "    data = copy.deepcopy(data)\n",
    "    ids = data.df_targets.sample(frac=1,random_state=random_state).index\n",
    "    data.df_featurized = data.df_featurized.loc[ids]\n",
    "    data.df_targets = data.df_targets.loc[ids]\n",
    "    data.df_structure = data.df_structure.loc[ids]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def MDKsplit(data,n_splits=5,random_state=10):\n",
    "    data = shuffle_MD(data,random_state=random_state)\n",
    "    ids = np.array(data.structure_ids)\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    folds = []\n",
    "    for train_idx, val_idx in kf.split(ids):\n",
    "        data_train = MODData(data.df_structure.iloc[train_idx]['structure'].values,data.df_targets.iloc[train_idx].values,target_names=data.df_targets.columns,structure_ids=ids[train_idx])\n",
    "        data_train.df_featurized = data.df_featurized.iloc[train_idx]\n",
    "        #data_train.optimal_features = data.optimal_features\n",
    "        \n",
    "        data_val = MODData(data.df_structure.iloc[val_idx]['structure'].values,data.df_targets.iloc[val_idx].values,target_names=data.df_targets.columns,structure_ids=ids[val_idx])\n",
    "        data_val.df_featurized = data.df_featurized.iloc[val_idx]\n",
    "        #data_val.optimal_features = data.optimal_features\n",
    "\n",
    "        folds.append((data_train,data_val))\n",
    "        \n",
    "    return folds\n",
    "\n",
    "def MD_append(md,lmd):\n",
    "    md = copy.deepcopy(md)\n",
    "    for m in lmd:\n",
    "        md.df_structure.append(m.df_structure)\n",
    "        md.df_targets.append(m.df_targets)\n",
    "        md.df_featurized.append(m.df_featurized)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:08:22.256194Z",
     "start_time": "2021-06-14T16:07:50.169148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "If you use the ChemEnv tool for your research, please consider citing the following reference(s) :\n",
      "==================================================================================================\n",
      "David Waroquiers, Xavier Gonze, Gian-Marco Rignanese, Cathrin Welker-Nieuwoudt, Frank Rosowski,\n",
      "Michael Goebel, Stephan Schenk, Peter Degelmann, Rute Andre, Robert Glaum, and Geoffroy Hautier,\n",
      "\"Statistical analysis of coordination environments in oxides\",\n",
      "Chem. Mater., 2017, 29 (19), pp 8346-8360,\n",
      "DOI: 10.1021/acs.chemmater.7b02766\n",
      "\n",
      "2021-06-14 18:07:56,566 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc0d46bd460> object, created with modnet version <=0.1.7\n",
      "2021-06-14 18:08:14,878 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc0d46bd580> object, created with modnet version <=0.1.7\n",
      "2021-06-14 18:08:20,436 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc0cda2ceb0> object, created with modnet version 0.1.8~develop\n",
      "2021-06-14 18:08:22,240 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc0d6a693d0> object, created with modnet version <=0.1.7\n"
     ]
    }
   ],
   "source": [
    "md_exp = MODData.load('exp_gap_all')\n",
    "md_exp.df_targets.columns = ['gap']\n",
    "md_pbe = MODData.load('pbe_gap.zip')\n",
    "md_pbe.df_targets.columns = ['gap']\n",
    "md_hse = MODData.load('hse_gap.zip')\n",
    "md_hse.df_targets.columns = ['gap']\n",
    "\n",
    "md_joint = MODData.load('exp_pbe_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:08:22.270846Z",
     "start_time": "2021-06-14T16:08:22.262261Z"
    }
   },
   "outputs": [],
   "source": [
    "md_joint.df_targets = md_joint.df_targets.drop(columns=['pbe_gap', 'difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T16:08:22.360373Z",
     "start_time": "2021-06-14T16:08:22.274913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mp-12699</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-559459</th>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-21162</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1306</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-15252</th>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-556541</th>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-2602</th>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-30366</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-3718</th>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-3709</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1703 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           exp_gap\n",
       "mp-12699      0.00\n",
       "mp-559459     3.40\n",
       "mp-21162      0.00\n",
       "mp-1306       0.00\n",
       "mp-15252      0.53\n",
       "...            ...\n",
       "mp-556541     1.80\n",
       "mp-2602       1.04\n",
       "mp-30366      0.00\n",
       "mp-3718       1.45\n",
       "mp-3709       0.00\n",
       "\n",
       "[1703 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_joint.df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.125370Z",
     "start_time": "2021-06-14T16:08:22.376415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 18:08:24,048 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,127 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,153 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,219 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,259 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,465 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,499 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,636 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,700 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:24,790 - modnet - INFO - Loaded DeBreuck2020Featurizer featurizer.\n",
      "2021-06-14 18:08:25,695 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fc03e4aac10> object, created with modnet version 0.1.9\n",
      "2021-06-14 18:08:33,853 - modnet - INFO - Generation number 0\n",
      "2021-06-14 18:08:39,764 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [38:26<00:00, 23.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 18:47:06,715 - modnet - INFO - Loss per individual: ind 0: 0.621 \tind 1: 0.681 \tind 2: 0.639 \tind 3: 0.657 \tind 4: 0.601 \tind 5: 0.607 \tind 6: 0.641 \tind 7: 0.625 \tind 8: 0.627 \tind 9: 0.649 \tind 10: 0.646 \tind 11: 0.595 \tind 12: 0.658 \tind 13: 0.624 \tind 14: 0.713 \tind 15: 0.728 \tind 16: 0.623 \tind 17: 0.638 \tind 18: 0.608 \tind 19: 0.966 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 18:47:09,949 - modnet - INFO - Generation number 1\n",
      "2021-06-14 18:47:17,051 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [34:07<00:00, 20.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 19:21:24,146 - modnet - INFO - Loss per individual: ind 0: 0.611 \tind 1: 0.601 \tind 2: 0.610 \tind 3: 0.604 \tind 4: 0.598 \tind 5: 0.619 \tind 6: 0.640 \tind 7: 0.625 \tind 8: 0.762 \tind 9: 0.617 \tind 10: 0.673 \tind 11: 0.619 \tind 12: 0.664 \tind 13: 0.645 \tind 14: 0.642 \tind 15: 0.586 \tind 16: 0.617 \tind 17: 0.621 \tind 18: 0.618 \tind 19: 0.598 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 19:21:27,509 - modnet - INFO - Generation number 2\n",
      "2021-06-14 19:21:37,786 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [32:33<00:00, 19.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 19:54:11,313 - modnet - INFO - Loss per individual: ind 0: 0.606 \tind 1: 0.648 \tind 2: 0.607 \tind 3: 0.653 \tind 4: 0.667 \tind 5: 0.651 \tind 6: 0.650 \tind 7: 0.601 \tind 8: 0.608 \tind 9: 0.620 \tind 10: 0.581 \tind 11: 0.591 \tind 12: 0.659 \tind 13: 0.651 \tind 14: 0.640 \tind 15: 0.676 \tind 16: 0.660 \tind 17: 0.708 \tind 18: 0.644 \tind 19: 0.675 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 19:54:13,902 - modnet - INFO - Generation number 3\n",
      "2021-06-14 19:54:21,125 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [34:10<00:00, 20.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 20:28:31,524 - modnet - INFO - Loss per individual: ind 0: 0.654 \tind 1: 0.615 \tind 2: 0.621 \tind 3: 0.654 \tind 4: 0.655 \tind 5: 0.663 \tind 6: 0.625 \tind 7: 0.664 \tind 8: 0.644 \tind 9: 0.616 \tind 10: 0.598 \tind 11: 0.723 \tind 12: 0.663 \tind 13: 0.625 \tind 14: 0.602 \tind 15: 0.628 \tind 16: 0.609 \tind 17: 0.583 \tind 18: 0.626 \tind 19: 0.584 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 20:28:35,169 - modnet - INFO - Generation number 4\n",
      "2021-06-14 20:28:45,532 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [34:18<00:00, 20.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 21:03:04,594 - modnet - INFO - Loss per individual: ind 0: 0.622 \tind 1: 0.582 \tind 2: 0.603 \tind 3: 0.695 \tind 4: 0.627 \tind 5: 0.618 \tind 6: 0.652 \tind 7: 0.651 \tind 8: 0.669 \tind 9: 0.644 \tind 10: 0.651 \tind 11: 0.639 \tind 12: 0.665 \tind 13: 0.608 \tind 14: 0.609 \tind 15: 0.589 \tind 16: 0.618 \tind 17: 0.622 \tind 18: 0.620 \tind 19: 0.731 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 21:03:07,900 - modnet - INFO - Generation number 5\n",
      "2021-06-14 21:03:19,235 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [37:30<00:00, 22.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 21:40:50,081 - modnet - INFO - Loss per individual: ind 0: 0.600 \tind 1: 0.651 \tind 2: 0.666 \tind 3: 0.678 \tind 4: 0.627 \tind 5: 0.639 \tind 6: 0.593 \tind 7: 0.594 \tind 8: 0.604 \tind 9: 0.675 \tind 10: 0.708 \tind 11: 0.610 \tind 12: 0.666 \tind 13: 0.624 \tind 14: 0.599 \tind 15: 0.592 \tind 16: 0.595 \tind 17: 0.640 \tind 18: 0.664 \tind 19: 0.639 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 21:40:53,606 - modnet - INFO - Early stopping: same best model for 4 consecutive generations\n",
      "mae_ph1\n",
      "0.37426384649387107\n",
      "2021-06-14 21:41:12,596 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-14 21:41:18,178 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00250: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00270: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00290: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00310: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00339: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00359: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00379: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00399: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00419: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00439: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00459: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00479: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00499: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00519: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00539: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00559: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00579: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00599: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00624: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00644: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00664: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00684: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00704: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00724: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00744: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00764: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00784: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00821: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00841: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00861: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00881: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00901: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00921: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00941: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00961: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00981: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "2021-06-14 21:44:18,439 - modnet - INFO - loss: 0.2713\texp_gap_loss: 0.1668\tpbe_gap_loss: 0.1045\texp_gap_mae: 0.1668\tpbe_gap_mae: 0.1045\tlr: 0.0000\t\n",
      "2021-06-14 21:44:18,441 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00256: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00276: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00296: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00318: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00338: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00376: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00396: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00416: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00436: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00456: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00476: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00496: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00516: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00536: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00556: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00576: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00596: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00616: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00636: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00656: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00656: early stopping\n",
      "2021-06-14 21:45:25,561 - modnet - INFO - loss: 0.2190\texp_gap_loss: 0.1367\tpbe_gap_loss: 0.0823\texp_gap_mae: 0.1367\tpbe_gap_mae: 0.0823\tlr: 0.0000\t\n",
      "2021-06-14 21:45:25,562 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00264: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00295: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00455: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00475: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00495: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00515: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00535: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00555: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00575: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00575: early stopping\n",
      "2021-06-14 21:46:26,088 - modnet - INFO - loss: 0.2407\texp_gap_loss: 0.1446\tpbe_gap_loss: 0.0961\texp_gap_mae: 0.1446\tpbe_gap_mae: 0.0961\tlr: 0.0000\t\n",
      "2021-06-14 21:46:26,090 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00221: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00330: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00350: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00370: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00390: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00425: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00462: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00482: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00502: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00522: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00542: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00562: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00582: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00602: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00622: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00642: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00662: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00682: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00702: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00722: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00742: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00742: early stopping\n",
      "2021-06-14 21:51:42,956 - modnet - INFO - loss: 0.2623\texp_gap_loss: 0.1565\tpbe_gap_loss: 0.1058\texp_gap_mae: 0.1565\tpbe_gap_mae: 0.1058\tlr: 0.0000\t\n",
      "2021-06-14 21:51:42,958 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00317: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00337: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00357: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00377: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00417: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00437: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00457: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00477: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00497: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00517: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00537: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00557: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00577: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00597: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00597: early stopping\n",
      "2021-06-14 21:53:29,321 - modnet - INFO - loss: 0.2955\texp_gap_loss: 0.1630\tpbe_gap_loss: 0.1325\texp_gap_mae: 0.1630\tpbe_gap_mae: 0.1325\tlr: 0.0000\t\n",
      "mae_ph2\n",
      "0.4403460104980106\n",
      "2021-06-14 21:53:31,816 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-14 21:53:35,540 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00344: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00376: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00396: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00416: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00436: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00456: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00476: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00496: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00516: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00536: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00556: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00576: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00596: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00616: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00653: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00656: early stopping\n",
      "2021-06-14 21:55:31,053 - modnet - INFO - loss: 0.2282\texp_gap_loss: 0.1393\tpbe_gap_loss: 0.0890\texp_gap_mae: 0.1393\tpbe_gap_mae: 0.0890\tlr: 0.0000\t\n",
      "2021-06-14 21:55:31,064 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00295: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00435: early stopping\n",
      "2021-06-14 21:56:46,726 - modnet - INFO - loss: 0.2439\texp_gap_loss: 0.1379\tpbe_gap_loss: 0.1061\texp_gap_mae: 0.1379\tpbe_gap_mae: 0.1061\tlr: 0.0000\t\n",
      "2021-06-14 21:56:46,727 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00324: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00344: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00364: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00384: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00404: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00424: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00444: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00464: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00484: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00485: early stopping\n",
      "2021-06-14 21:58:11,690 - modnet - INFO - loss: 0.2564\texp_gap_loss: 0.1440\tpbe_gap_loss: 0.1124\texp_gap_mae: 0.1440\tpbe_gap_mae: 0.1124\tlr: 0.0000\t\n",
      "2021-06-14 21:58:11,694 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00240: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00363: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00383: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00403: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00423: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00443: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00540: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00560: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00584: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00604: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00624: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00646: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00666: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00686: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00706: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00726: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00746: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00766: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00786: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00806: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00826: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00846: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00864: early stopping\n",
      "2021-06-14 22:00:18,336 - modnet - INFO - loss: 0.2101\texp_gap_loss: 0.1198\tpbe_gap_loss: 0.0904\texp_gap_mae: 0.1198\tpbe_gap_mae: 0.0904\tlr: 0.0000\t\n",
      "2021-06-14 22:00:18,338 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00295: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00464: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00484: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00504: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00528: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00548: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00568: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00588: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00608: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00628: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00648: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00677: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00697: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00717: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00737: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00757: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00777: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00797: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00817: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00837: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00857: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00877: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00897: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00917: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "\n",
      "Epoch 00937: ReduceLROnPlateau reducing learning rate to 4.5474734072206875e-15.\n",
      "\n",
      "Epoch 00957: ReduceLROnPlateau reducing learning rate to 2.2737367036103438e-15.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00957: early stopping\n",
      "2021-06-14 22:01:52,283 - modnet - INFO - loss: 0.2885\texp_gap_loss: 0.1633\tpbe_gap_loss: 0.1252\texp_gap_mae: 0.1633\tpbe_gap_mae: 0.1252\tlr: 0.0000\t\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.20it/s]\n",
      " 10%|█         | 1/10 [00:00<00:01,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           0.437\n",
      "  RMSE          0.924\n",
      "  MDAE          0.079\n",
      "  MARPD         136.246\n",
      "  R2            0.669\n",
      "  Correlation   0.833\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.182\n",
      "  Mean-absolute Calibration Error       0.166\n",
      "  Miscalibration Area                   0.168\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.248\n",
      "     Group Size: 0.56 -- Calibration Error: 0.191\n",
      "     Group Size: 1.00 -- Calibration Error: 0.166\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.283\n",
      "     Group Size: 0.56 -- Calibration Error: 0.204\n",
      "     Group Size: 1.00 -- Calibration Error: 0.182\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   0.304\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   124.926\n",
      "  CRPS                      0.368\n",
      "  Check Score               0.185\n",
      "  Interval Score            2.780\n",
      "MAE =\n",
      "0.43693347197395244\n",
      "uncertainty =\n",
      "0.16648124\n",
      "2021-06-14 22:02:31,595 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fbdc15c9d00> object, created with modnet version 0.1.9\n",
      "2021-06-14 22:02:40,886 - modnet - INFO - Generation number 0\n",
      "2021-06-14 22:02:49,962 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:06:55<00:00, 40.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 23:09:45,285 - modnet - INFO - Loss per individual: ind 0: 0.706 \tind 1: 0.705 \tind 2: 0.628 \tind 3: 0.721 \tind 4: 0.617 \tind 5: 0.607 \tind 6: 0.641 \tind 7: 0.602 \tind 8: 0.669 \tind 9: 0.625 \tind 10: 0.724 \tind 11: 0.753 \tind 12: 0.639 \tind 13: 0.669 \tind 14: 0.793 \tind 15: 0.625 \tind 16: 0.578 \tind 17: 0.600 \tind 18: 0.741 \tind 19: 0.593 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 23:09:48,314 - modnet - INFO - Generation number 1\n",
      "2021-06-14 23:09:57,635 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [28:45<00:00, 17.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 23:38:43,380 - modnet - INFO - Loss per individual: ind 0: 0.641 \tind 1: 0.754 \tind 2: 0.605 \tind 3: 0.592 \tind 4: 0.683 \tind 5: 0.695 \tind 6: 0.613 \tind 7: 0.608 \tind 8: 0.625 \tind 9: 0.605 \tind 10: 0.593 \tind 11: 0.794 \tind 12: 0.668 \tind 13: 0.620 \tind 14: 0.607 \tind 15: 0.599 \tind 16: 0.638 \tind 17: 0.693 \tind 18: 0.593 \tind 19: 0.755 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14 23:38:47,164 - modnet - INFO - Generation number 2\n",
      "2021-06-14 23:38:57,424 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [32:10<00:00, 19.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 00:11:08,223 - modnet - INFO - Loss per individual: ind 0: 0.605 \tind 1: 0.635 \tind 2: 0.607 \tind 3: 0.628 \tind 4: 0.622 \tind 5: 0.599 \tind 6: 0.595 \tind 7: 0.616 \tind 8: 0.612 \tind 9: 0.625 \tind 10: 0.615 \tind 11: 0.588 \tind 12: 0.722 \tind 13: 0.621 \tind 14: 0.626 \tind 15: 0.639 \tind 16: 0.598 \tind 17: 0.611 \tind 18: 0.682 \tind 19: 0.656 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 00:11:11,712 - modnet - INFO - Generation number 3\n",
      "2021-06-15 00:11:22,213 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [30:01<00:00, 18.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 00:41:24,188 - modnet - INFO - Loss per individual: ind 0: 0.663 \tind 1: 0.610 \tind 2: 0.634 \tind 3: 0.665 \tind 4: 0.602 \tind 5: 0.589 \tind 6: 0.661 \tind 7: 0.634 \tind 8: 0.603 \tind 9: 0.665 \tind 10: 0.648 \tind 11: 0.758 \tind 12: 0.616 \tind 13: 0.635 \tind 14: 0.582 \tind 15: 0.623 \tind 16: 0.660 \tind 17: 0.608 \tind 18: 0.613 \tind 19: 0.566 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 00:41:27,542 - modnet - INFO - Generation number 4\n",
      "2021-06-15 00:41:40,630 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [30:39<00:00, 18.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 01:12:20,532 - modnet - INFO - Loss per individual: ind 0: 0.635 \tind 1: 0.801 \tind 2: 0.594 \tind 3: 0.603 \tind 4: 0.673 \tind 5: 0.584 \tind 6: 0.618 \tind 7: 0.604 \tind 8: 0.598 \tind 9: 0.650 \tind 10: 0.615 \tind 11: 0.619 \tind 12: 0.616 \tind 13: 0.635 \tind 14: 0.632 \tind 15: 0.593 \tind 16: 0.615 \tind 17: 0.596 \tind 18: 0.591 \tind 19: 0.617 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 01:12:24,196 - modnet - INFO - Generation number 5\n",
      "2021-06-15 01:12:37,265 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [28:00<00:00, 16.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 01:40:38,139 - modnet - INFO - Loss per individual: ind 0: 0.598 \tind 1: 0.653 \tind 2: 0.595 \tind 3: 0.596 \tind 4: 0.585 \tind 5: 0.606 \tind 6: 0.620 \tind 7: 0.602 \tind 8: 0.577 \tind 9: 0.599 \tind 10: 0.592 \tind 11: 0.621 \tind 12: 0.634 \tind 13: 0.617 \tind 14: 0.602 \tind 15: 0.629 \tind 16: 0.668 \tind 17: 0.627 \tind 18: 0.636 \tind 19: 0.595 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 01:40:40,991 - modnet - INFO - Generation number 6\n",
      "2021-06-15 01:40:52,525 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [31:59<00:00, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 02:12:52,011 - modnet - INFO - Loss per individual: ind 0: 0.644 \tind 1: 0.622 \tind 2: 0.594 \tind 3: 0.614 \tind 4: 0.578 \tind 5: 0.601 \tind 6: 0.647 \tind 7: 0.605 \tind 8: 0.605 \tind 9: 0.655 \tind 10: 0.664 \tind 11: 0.661 \tind 12: 0.670 \tind 13: 0.609 \tind 14: 0.624 \tind 15: 0.650 \tind 16: 0.594 \tind 17: 0.598 \tind 18: 0.626 \tind 19: 0.597 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 02:12:55,634 - modnet - INFO - Early stopping: same best model for 4 consecutive generations\n",
      "mae_ph1\n",
      "0.37195475799303\n",
      "2021-06-15 02:13:14,689 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 02:13:19,353 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00337: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00357: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00377: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00417: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00437: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00457: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00477: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00497: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00517: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00537: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00557: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00577: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00597: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00617: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00617: early stopping\n",
      "2021-06-15 02:15:59,939 - modnet - INFO - loss: 0.3041\texp_gap_loss: 0.1843\tpbe_gap_loss: 0.1198\texp_gap_mae: 0.1843\tpbe_gap_mae: 0.1198\tlr: 0.0000\t\n",
      "2021-06-15 02:15:59,941 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00348: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00368: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00388: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00408: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00428: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00448: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00468: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00503: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00523: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00543: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00563: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00583: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00603: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00623: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00643: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00663: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00683: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00703: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00723: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00743: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00763: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00783: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00783: early stopping\n",
      "2021-06-15 02:18:57,597 - modnet - INFO - loss: 0.2696\texp_gap_loss: 0.1604\tpbe_gap_loss: 0.1091\texp_gap_mae: 0.1604\tpbe_gap_mae: 0.1091\tlr: 0.0000\t\n",
      "2021-06-15 02:18:57,598 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00298: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00331: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00351: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00381: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00401: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00421: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00441: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00461: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00481: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00501: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00521: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00541: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00561: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00581: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00601: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00621: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00641: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00661: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00661: early stopping\n",
      "2021-06-15 02:21:30,442 - modnet - INFO - loss: 0.3074\texp_gap_loss: 0.1746\tpbe_gap_loss: 0.1327\texp_gap_mae: 0.1746\tpbe_gap_mae: 0.1327\tlr: 0.0000\t\n",
      "2021-06-15 02:21:30,444 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00221: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00363: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00383: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00403: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00423: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00443: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00463: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00483: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00501: early stopping\n",
      "2021-06-15 02:23:37,288 - modnet - INFO - loss: 0.2535\texp_gap_loss: 0.1442\tpbe_gap_loss: 0.1093\texp_gap_mae: 0.1442\tpbe_gap_mae: 0.1093\tlr: 0.0000\t\n",
      "2021-06-15 02:23:37,289 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00201: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00221: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00261: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00281: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00301: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00321: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00341: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00371: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00391: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00411: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00431: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00451: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00471: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00491: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00511: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00531: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00551: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00571: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00591: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00611: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00631: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00651: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00651: early stopping\n",
      "2021-06-15 02:26:24,752 - modnet - INFO - loss: 0.3359\texp_gap_loss: 0.1925\tpbe_gap_loss: 0.1434\texp_gap_mae: 0.1925\tpbe_gap_mae: 0.1434\tlr: 0.0000\t\n",
      "mae_ph2\n",
      "0.3900658344863325\n",
      "2021-06-15 02:26:27,104 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 02:26:31,797 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00338: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00358: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00378: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00398: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00418: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00438: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00458: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00478: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00498: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00518: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00538: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00558: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00578: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00598: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00618: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00618: early stopping\n",
      "2021-06-15 02:29:03,960 - modnet - INFO - loss: 0.2219\texp_gap_loss: 0.1253\tpbe_gap_loss: 0.0966\texp_gap_mae: 0.1253\tpbe_gap_mae: 0.0966\tlr: 0.0000\t\n",
      "2021-06-15 02:29:03,963 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00226: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00246: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00266: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00296: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00316: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00338: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00358: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00378: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00398: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00418: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00438: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00458: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00478: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00498: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00518: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00538: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00558: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00578: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00598: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00638: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00658: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00678: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00698: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00718: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00738: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00758: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00778: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00798: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00818: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00838: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00858: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00878: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00898: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "\n",
      "Epoch 00918: ReduceLROnPlateau reducing learning rate to 4.5474734072206875e-15.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00918: early stopping\n",
      "2021-06-15 02:32:22,618 - modnet - INFO - loss: 0.2613\texp_gap_loss: 0.1549\tpbe_gap_loss: 0.1064\texp_gap_mae: 0.1549\tpbe_gap_mae: 0.1064\tlr: 0.0000\t\n",
      "2021-06-15 02:32:22,620 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00201: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00226: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00305: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00345: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00385: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00405: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00425: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00445: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00465: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00503: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00523: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00543: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00563: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00583: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00603: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00623: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00643: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00663: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00686: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00706: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00726: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00765: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00785: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00805: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00825: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00845: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00865: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00885: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00905: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00925: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00945: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "\n",
      "Epoch 00965: ReduceLROnPlateau reducing learning rate to 4.5474734072206875e-15.\n",
      "\n",
      "Epoch 00985: ReduceLROnPlateau reducing learning rate to 2.2737367036103438e-15.\n",
      "2021-06-15 02:36:18,228 - modnet - INFO - loss: 0.2692\texp_gap_loss: 0.1644\tpbe_gap_loss: 0.1048\texp_gap_mae: 0.1644\tpbe_gap_mae: 0.1048\tlr: 0.0000\t\n",
      "2021-06-15 02:36:18,231 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00227: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00267: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00287: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00307: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00327: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00347: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00367: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00387: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00407: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00441: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00461: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00481: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00501: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00521: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00541: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00561: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00581: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00601: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00621: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00641: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00661: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00681: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00701: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00721: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00721: early stopping\n",
      "2021-06-15 02:39:24,051 - modnet - INFO - loss: 0.2602\texp_gap_loss: 0.1460\tpbe_gap_loss: 0.1142\texp_gap_mae: 0.1460\tpbe_gap_mae: 0.1142\tlr: 0.0000\t\n",
      "2021-06-15 02:39:24,054 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00239: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00319: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00339: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00359: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00379: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00399: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00419: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00439: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00459: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00479: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00499: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00519: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00519: early stopping\n",
      "2021-06-15 02:41:41,399 - modnet - INFO - loss: 0.2806\texp_gap_loss: 0.1597\tpbe_gap_loss: 0.1209\texp_gap_mae: 0.1597\tpbe_gap_mae: 0.1209\tlr: 0.0000\t\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.34it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           0.399\n",
      "  RMSE          0.894\n",
      "  MDAE          0.066\n",
      "  MARPD         135.334\n",
      "  R2            0.683\n",
      "  Correlation   0.831\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.244\n",
      "  Mean-absolute Calibration Error       0.228\n",
      "  Miscalibration Area                   0.231\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.283\n",
      "     Group Size: 0.56 -- Calibration Error: 0.246\n",
      "     Group Size: 1.00 -- Calibration Error: 0.228\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.312\n",
      "     Group Size: 0.56 -- Calibration Error: 0.261\n",
      "     Group Size: 1.00 -- Calibration Error: 0.244\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   0.255\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   133.155\n",
      "  CRPS                      0.336\n",
      "  Check Score               0.169\n",
      "  Interval Score            2.539\n",
      "MAE =\n",
      "0.3993549019144095\n",
      "uncertainty =\n",
      "0.14064462\n",
      "2021-06-15 02:41:54,872 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fbe6c748d00> object, created with modnet version 0.1.9\n",
      "2021-06-15 02:42:05,443 - modnet - INFO - Generation number 0\n",
      "2021-06-15 02:42:15,418 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [32:01<00:00, 19.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 03:14:16,480 - modnet - INFO - Loss per individual: ind 0: 0.621 \tind 1: 0.755 \tind 2: 0.672 \tind 3: 0.699 \tind 4: 0.703 \tind 5: 0.778 \tind 6: 0.707 \tind 7: 0.681 \tind 8: 0.754 \tind 9: 0.637 \tind 10: 0.662 \tind 11: 0.683 \tind 12: 0.719 \tind 13: 0.814 \tind 14: 0.749 \tind 15: 0.664 \tind 16: 0.713 \tind 17: 0.658 \tind 18: 0.751 \tind 19: 0.647 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 03:14:19,957 - modnet - INFO - Generation number 1\n",
      "2021-06-15 03:14:31,016 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [33:41<00:00, 20.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 03:48:12,243 - modnet - INFO - Loss per individual: ind 0: 0.693 \tind 1: 0.697 \tind 2: 0.772 \tind 3: 0.630 \tind 4: 0.685 \tind 5: 0.622 \tind 6: 0.681 \tind 7: 0.729 \tind 8: 0.688 \tind 9: 0.682 \tind 10: 0.689 \tind 11: 0.658 \tind 12: 0.746 \tind 13: 0.665 \tind 14: 0.639 \tind 15: 0.680 \tind 16: 0.645 \tind 17: 0.683 \tind 18: 0.656 \tind 19: 0.626 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 03:48:15,901 - modnet - INFO - Generation number 2\n",
      "2021-06-15 03:48:28,182 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [32:45<00:00, 19.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 04:21:13,548 - modnet - INFO - Loss per individual: ind 0: 0.776 \tind 1: 0.652 \tind 2: 0.649 \tind 3: 0.708 \tind 4: 0.643 \tind 5: 0.798 \tind 6: 0.672 \tind 7: 0.706 \tind 8: 0.728 \tind 9: 0.691 \tind 10: 0.664 \tind 11: 0.704 \tind 12: 0.714 \tind 13: 0.681 \tind 14: 0.613 \tind 15: 0.724 \tind 16: 0.707 \tind 17: 0.699 \tind 18: 0.819 \tind 19: 0.712 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 04:21:16,982 - modnet - INFO - Generation number 3\n",
      "2021-06-15 04:21:28,638 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [27:20<00:00, 16.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 04:48:49,552 - modnet - INFO - Loss per individual: ind 0: 0.670 \tind 1: 0.677 \tind 2: 0.689 \tind 3: 0.628 \tind 4: 0.686 \tind 5: 0.670 \tind 6: 0.685 \tind 7: 0.711 \tind 8: 0.673 \tind 9: 0.751 \tind 10: 0.676 \tind 11: 0.673 \tind 12: 0.647 \tind 13: 0.638 \tind 14: 0.707 \tind 15: 0.622 \tind 16: 0.683 \tind 17: 0.721 \tind 18: 0.659 \tind 19: 0.752 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 04:48:52,815 - modnet - INFO - Generation number 4\n",
      "2021-06-15 04:49:04,967 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:39<00:00, 17.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 05:18:45,260 - modnet - INFO - Loss per individual: ind 0: 0.630 \tind 1: 0.617 \tind 2: 0.663 \tind 3: 0.673 \tind 4: 0.639 \tind 5: 0.698 \tind 6: 0.675 \tind 7: 0.638 \tind 8: 0.739 \tind 9: 0.659 \tind 10: 0.675 \tind 11: 0.624 \tind 12: 0.626 \tind 13: 0.681 \tind 14: 0.689 \tind 15: 0.729 \tind 16: 0.712 \tind 17: 0.630 \tind 18: 0.641 \tind 19: 0.658 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 05:18:47,151 - modnet - INFO - Generation number 5\n",
      "2021-06-15 05:18:56,044 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:31<00:00, 17.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 05:48:28,226 - modnet - INFO - Loss per individual: ind 0: 0.764 \tind 1: 0.640 \tind 2: 0.730 \tind 3: 0.616 \tind 4: 0.722 \tind 5: 0.620 \tind 6: 0.652 \tind 7: 0.644 \tind 8: 0.618 \tind 9: 0.723 \tind 10: 0.716 \tind 11: 0.665 \tind 12: 0.675 \tind 13: 0.625 \tind 14: 0.721 \tind 15: 0.694 \tind 16: 0.648 \tind 17: 0.681 \tind 18: 0.651 \tind 19: 0.722 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 05:48:30,238 - modnet - INFO - Early stopping: same best model for 4 consecutive generations\n",
      "mae_ph1\n",
      "0.29872922113375855\n",
      "2021-06-15 05:48:40,816 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 05:48:44,376 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00227: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00267: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00319: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00339: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00359: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00379: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00399: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00419: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00439: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00459: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00479: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00499: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00519: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00539: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00559: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00579: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00579: early stopping\n",
      "2021-06-15 05:50:36,199 - modnet - INFO - loss: 0.2783\texp_gap_loss: 0.1535\tpbe_gap_loss: 0.1248\texp_gap_mae: 0.1535\tpbe_gap_mae: 0.1248\tlr: 0.0000\t\n",
      "2021-06-15 05:50:36,201 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00198: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00218: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00258: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00345: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00396: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00416: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00436: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00456: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00476: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00496: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00516: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00536: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00556: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00576: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00596: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00616: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00636: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00656: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00676: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00676: early stopping\n",
      "2021-06-15 05:52:46,191 - modnet - INFO - loss: 0.3009\texp_gap_loss: 0.1633\tpbe_gap_loss: 0.1375\texp_gap_mae: 0.1633\tpbe_gap_mae: 0.1375\tlr: 0.0000\t\n",
      "2021-06-15 05:52:46,193 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00333: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00353: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00373: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00393: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00413: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00433: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00453: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00473: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00473: early stopping\n",
      "2021-06-15 05:54:00,759 - modnet - INFO - loss: 0.2967\texp_gap_loss: 0.1816\tpbe_gap_loss: 0.1151\texp_gap_mae: 0.1816\tpbe_gap_mae: 0.1151\tlr: 0.0000\t\n",
      "2021-06-15 05:54:00,760 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00219: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00239: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00319: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00339: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00359: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00381: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00401: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00421: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00441: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00461: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00481: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00501: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00521: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00541: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00561: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00581: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00601: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00621: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00641: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00661: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00661: early stopping\n",
      "2021-06-15 05:56:10,037 - modnet - INFO - loss: 0.2905\texp_gap_loss: 0.1650\tpbe_gap_loss: 0.1254\texp_gap_mae: 0.1650\tpbe_gap_mae: 0.1254\tlr: 0.0000\t\n",
      "2021-06-15 05:56:10,048 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00246: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00305: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00345: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00400: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00420: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00440: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00560: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00580: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00600: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00620: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00641: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00661: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00681: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00701: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00721: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00741: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00761: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00781: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00801: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00821: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00841: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00861: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00881: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00901: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00921: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00921: early stopping\n",
      "2021-06-15 05:59:14,593 - modnet - INFO - loss: 0.2581\texp_gap_loss: 0.1603\tpbe_gap_loss: 0.0978\texp_gap_mae: 0.1603\tpbe_gap_mae: 0.0978\tlr: 0.0000\t\n",
      "mae_ph2\n",
      "0.29797288187585846\n",
      "2021-06-15 05:59:17,171 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 05:59:23,308 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00303: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00363: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00383: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00403: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00423: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00443: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00463: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00463: early stopping\n",
      "2021-06-15 06:00:58,283 - modnet - INFO - loss: 0.2971\texp_gap_loss: 0.1849\tpbe_gap_loss: 0.1121\texp_gap_mae: 0.1849\tpbe_gap_mae: 0.1121\tlr: 0.0000\t\n",
      "2021-06-15 06:00:58,285 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00220: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00240: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00264: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00284: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00305: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00345: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00385: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00405: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00425: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00445: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00465: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00485: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00505: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00525: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00545: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00565: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00585: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00585: early stopping\n",
      "2021-06-15 06:02:52,418 - modnet - INFO - loss: 0.3272\texp_gap_loss: 0.1783\tpbe_gap_loss: 0.1490\texp_gap_mae: 0.1783\tpbe_gap_mae: 0.1490\tlr: 0.0000\t\n",
      "2021-06-15 06:02:52,420 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00258: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00316: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00350: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00370: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00390: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00410: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00430: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00450: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00470: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00490: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00510: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00530: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00550: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00570: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00590: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00610: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00630: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00630: early stopping\n",
      "2021-06-15 06:04:12,662 - modnet - INFO - loss: 0.3307\texp_gap_loss: 0.1946\tpbe_gap_loss: 0.1361\texp_gap_mae: 0.1946\tpbe_gap_mae: 0.1361\tlr: 0.0000\t\n",
      "2021-06-15 06:04:12,663 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00348: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00368: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00388: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00408: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00444: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00464: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00484: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00504: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00524: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00544: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00564: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00584: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00604: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00624: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00644: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00664: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00684: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00704: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00724: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00724: early stopping\n",
      "2021-06-15 06:08:20,237 - modnet - INFO - loss: 0.3229\texp_gap_loss: 0.1929\tpbe_gap_loss: 0.1300\texp_gap_mae: 0.1929\tpbe_gap_mae: 0.1300\tlr: 0.0000\t\n",
      "2021-06-15 06:08:20,239 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00274: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00294: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00314: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00455: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00492: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00512: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00532: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00552: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00572: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00592: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00612: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00638: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00658: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00678: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00698: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00718: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00738: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00758: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00779: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00799: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00819: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00846: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00866: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00886: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "\n",
      "Epoch 00906: ReduceLROnPlateau reducing learning rate to 4.5474734072206875e-15.\n",
      "\n",
      "Epoch 00926: ReduceLROnPlateau reducing learning rate to 2.2737367036103438e-15.\n",
      "\n",
      "Epoch 00946: ReduceLROnPlateau reducing learning rate to 1.1368683518051719e-15.\n",
      "\n",
      "Epoch 00966: ReduceLROnPlateau reducing learning rate to 5.684341759025859e-16.\n",
      "\n",
      "Epoch 00986: ReduceLROnPlateau reducing learning rate to 2.8421708795129297e-16.\n",
      "2021-06-15 06:14:20,563 - modnet - INFO - loss: 0.2780\texp_gap_loss: 0.1611\tpbe_gap_loss: 0.1169\texp_gap_mae: 0.1611\tpbe_gap_mae: 0.1169\tlr: 0.0000\t\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.59it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           8282.757\n",
      "  RMSE          89922.036\n",
      "  MDAE          0.019\n",
      "  MARPD         138.450\n",
      "  R2            -3719372256.286\n",
      "  Correlation   -0.020\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.200\n",
      "  Mean-absolute Calibration Error       0.182\n",
      "  Miscalibration Area                   0.184\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.233\n",
      "     Group Size: 0.56 -- Calibration Error: 0.198\n",
      "     Group Size: 1.00 -- Calibration Error: 0.182\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.268\n",
      "     Group Size: 0.56 -- Calibration Error: 0.220\n",
      "     Group Size: 1.00 -- Calibration Error: 0.200\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   108986.289\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   89.715\n",
      "  CRPS                      4927.675\n",
      "  Check Score               2488.362\n",
      "  Interval Score            22572.994\n",
      "MAE =\n",
      "0.2955363847435911\n",
      "uncertainty =\n",
      "0.10966733\n",
      "2021-06-15 06:14:31,905 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fbde674a5e0> object, created with modnet version 0.1.9\n",
      "2021-06-15 06:14:41,976 - modnet - INFO - Generation number 0\n",
      "2021-06-15 06:14:52,343 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:52<00:00, 17.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 06:44:44,798 - modnet - INFO - Loss per individual: ind 0: 0.619 \tind 1: 0.672 \tind 2: 0.685 \tind 3: 0.623 \tind 4: 0.684 \tind 5: 0.990 \tind 6: 0.689 \tind 7: 0.685 \tind 8: 3.974 \tind 9: 0.868 \tind 10: 0.628 \tind 11: 0.722 \tind 12: 0.717 \tind 13: 0.644 \tind 14: 0.667 \tind 15: 1.018 \tind 16: 0.671 \tind 17: 0.641 \tind 18: 0.713 \tind 19: 3.546 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 06:44:48,183 - modnet - INFO - Generation number 1\n",
      "2021-06-15 06:45:00,379 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [32:08<00:00, 19.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 07:17:08,685 - modnet - INFO - Loss per individual: ind 0: 6.438 \tind 1: 0.692 \tind 2: 0.596 \tind 3: 0.649 \tind 4: 0.648 \tind 5: 0.638 \tind 6: 0.655 \tind 7: 0.652 \tind 8: 0.640 \tind 9: 0.623 \tind 10: 9.578 \tind 11: 0.681 \tind 12: 0.632 \tind 13: 1.589 \tind 14: 11.782 \tind 15: 0.674 \tind 16: 0.926 \tind 17: 10.803 \tind 18: 0.635 \tind 19: 0.614 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 07:17:11,614 - modnet - INFO - Generation number 2\n",
      "2021-06-15 07:17:23,559 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [44:26<00:00, 26.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 08:01:50,758 - modnet - INFO - Loss per individual: ind 0: 0.624 \tind 1: 0.689 \tind 2: 0.735 \tind 3: 0.623 \tind 4: 0.605 \tind 5: 0.622 \tind 6: 0.663 \tind 7: 0.603 \tind 8: 0.734 \tind 9: 0.704 \tind 10: 0.642 \tind 11: 0.627 \tind 12: 0.609 \tind 13: 0.613 \tind 14: 0.601 \tind 15: 0.679 \tind 16: 0.660 \tind 17: 0.611 \tind 18: 0.629 \tind 19: 0.645 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 08:01:52,712 - modnet - INFO - Generation number 3\n",
      "2021-06-15 08:02:01,361 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:21<00:00, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 08:31:23,089 - modnet - INFO - Loss per individual: ind 0: 0.635 \tind 1: 0.622 \tind 2: 0.648 \tind 3: 0.643 \tind 4: 0.665 \tind 5: 0.634 \tind 6: 0.642 \tind 7: 0.619 \tind 8: 0.650 \tind 9: 0.642 \tind 10: 0.623 \tind 11: 0.609 \tind 12: 0.638 \tind 13: 0.614 \tind 14: 0.640 \tind 15: 0.671 \tind 16: 79.691 \tind 17: 0.673 \tind 18: 0.597 \tind 19: 0.623 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 08:31:26,687 - modnet - INFO - Generation number 4\n",
      "2021-06-15 08:31:40,168 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [53:20<00:00, 32.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 09:25:00,497 - modnet - INFO - Loss per individual: ind 0: 0.601 \tind 1: 0.595 \tind 2: 0.679 \tind 3: 2.135 \tind 4: 0.644 \tind 5: 0.625 \tind 6: 0.639 \tind 7: 0.605 \tind 8: 0.613 \tind 9: 0.593 \tind 10: 0.609 \tind 11: 0.735 \tind 12: 0.636 \tind 13: 0.694 \tind 14: 0.652 \tind 15: 0.657 \tind 16: 0.610 \tind 17: 0.630 \tind 18: 0.627 \tind 19: 0.657 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 09:25:02,867 - modnet - INFO - Generation number 5\n",
      "2021-06-15 09:25:12,829 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [59:26<00:00, 35.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 10:24:39,613 - modnet - INFO - Loss per individual: ind 0: 0.596 \tind 1: 0.632 \tind 2: 0.613 \tind 3: 0.658 \tind 4: 0.635 \tind 5: 0.654 \tind 6: 0.605 \tind 7: 0.673 \tind 8: 0.628 \tind 9: 0.599 \tind 10: 0.610 \tind 11: 0.640 \tind 12: 0.627 \tind 13: 0.663 \tind 14: 0.602 \tind 15: 0.625 \tind 16: 0.625 \tind 17: 0.627 \tind 18: 0.632 \tind 19: 0.638 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 10:24:41,812 - modnet - INFO - Generation number 6\n",
      "2021-06-15 10:24:54,211 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [36:36<00:00, 21.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:01:30,377 - modnet - INFO - Loss per individual: ind 0: 0.601 \tind 1: 0.642 \tind 2: 0.630 \tind 3: 0.644 \tind 4: 0.623 \tind 5: 0.663 \tind 6: 0.641 \tind 7: 0.665 \tind 8: 0.688 \tind 9: 0.658 \tind 10: 0.625 \tind 11: 0.609 \tind 12: 0.654 \tind 13: 0.609 \tind 14: 0.661 \tind 15: 0.622 \tind 16: 0.600 \tind 17: 0.642 \tind 18: 0.620 \tind 19: 0.627 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:01:33,477 - modnet - INFO - Generation number 7\n",
      "2021-06-15 11:01:46,469 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:55<00:00, 17.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:31:42,224 - modnet - INFO - Loss per individual: ind 0: 0.642 \tind 1: 0.673 \tind 2: 0.602 \tind 3: 0.644 \tind 4: 0.647 \tind 5: 0.683 \tind 6: 0.618 \tind 7: 0.703 \tind 8: 0.619 \tind 9: 0.608 \tind 10: 0.650 \tind 11: 0.616 \tind 12: 0.620 \tind 13: 0.629 \tind 14: 0.639 \tind 15: 0.626 \tind 16: 0.634 \tind 17: 0.655 \tind 18: 0.632 \tind 19: 0.618 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:31:45,681 - modnet - INFO - Early stopping: same best model for 4 consecutive generations\n",
      "mae_ph1\n",
      "0.3424137206380952\n",
      "2021-06-15 11:32:08,503 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 11:32:14,260 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00222: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00268: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00296: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00316: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00336: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00356: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00376: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00396: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00428: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00448: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00468: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00488: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00508: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00528: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00548: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00568: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00588: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00608: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00628: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00648: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00668: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00688: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00708: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00708: early stopping\n",
      "2021-06-15 11:35:01,350 - modnet - INFO - loss: 0.2779\texp_gap_loss: 0.1503\tpbe_gap_loss: 0.1276\texp_gap_mae: 0.1503\tpbe_gap_mae: 0.1276\tlr: 0.0000\t\n",
      "2021-06-15 11:35:01,353 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00305: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00345: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00365: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00385: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00405: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00425: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00445: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00445: early stopping\n",
      "2021-06-15 11:36:29,835 - modnet - INFO - loss: 0.3041\texp_gap_loss: 0.1713\tpbe_gap_loss: 0.1328\texp_gap_mae: 0.1713\tpbe_gap_mae: 0.1328\tlr: 0.0000\t\n",
      "2021-06-15 11:36:29,838 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00331: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00351: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00371: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00391: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00411: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00431: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00451: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00471: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00491: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00511: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00511: early stopping\n",
      "2021-06-15 11:38:02,616 - modnet - INFO - loss: 0.2383\texp_gap_loss: 0.1368\tpbe_gap_loss: 0.1015\texp_gap_mae: 0.1368\tpbe_gap_mae: 0.1015\tlr: 0.0000\t\n",
      "2021-06-15 11:38:02,631 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00298: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00318: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00344: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00364: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00384: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00404: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00424: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00444: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00464: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00484: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00504: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00524: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00544: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00564: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00584: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00604: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00624: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00624: early stopping\n",
      "2021-06-15 11:40:19,146 - modnet - INFO - loss: 0.3306\texp_gap_loss: 0.1874\tpbe_gap_loss: 0.1432\texp_gap_mae: 0.1874\tpbe_gap_mae: 0.1432\tlr: 0.0000\t\n",
      "2021-06-15 11:40:19,151 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00170: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00331: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00351: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00371: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00400: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00420: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00440: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00540: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00560: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00580: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00600: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00620: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00640: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00660: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00684: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00704: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00724: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00744: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00764: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00784: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00804: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00824: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00844: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00864: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00884: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00904: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00924: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00944: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00964: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00964: early stopping\n",
      "2021-06-15 11:43:57,184 - modnet - INFO - loss: 0.2918\texp_gap_loss: 0.1690\tpbe_gap_loss: 0.1228\texp_gap_mae: 0.1690\tpbe_gap_mae: 0.1228\tlr: 0.0000\t\n",
      "mae_ph2\n",
      "0.36582671564505165\n",
      "2021-06-15 11:43:59,134 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 11:44:03,418 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00212: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00232: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00252: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00272: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00292: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00312: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00332: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00354: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00374: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00394: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00414: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00434: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00454: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00474: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00494: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00514: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00534: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00554: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00574: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00594: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00614: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00634: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00634: early stopping\n",
      "2021-06-15 11:46:27,450 - modnet - INFO - loss: 0.2902\texp_gap_loss: 0.1707\tpbe_gap_loss: 0.1195\texp_gap_mae: 0.1707\tpbe_gap_mae: 0.1195\tlr: 0.0000\t\n",
      "2021-06-15 11:46:27,452 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00295: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00455: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00475: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00495: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00495: early stopping\n",
      "2021-06-15 11:48:19,413 - modnet - INFO - loss: 0.3361\texp_gap_loss: 0.2026\tpbe_gap_loss: 0.1335\texp_gap_mae: 0.2026\tpbe_gap_mae: 0.1335\tlr: 0.0000\t\n",
      "2021-06-15 11:48:19,415 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00240: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00260: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00280: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00300: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00320: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00360: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00380: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00400: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00420: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00440: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00520: early stopping\n",
      "2021-06-15 11:50:16,504 - modnet - INFO - loss: 0.3210\texp_gap_loss: 0.2011\tpbe_gap_loss: 0.1200\texp_gap_mae: 0.2011\tpbe_gap_mae: 0.1200\tlr: 0.0000\t\n",
      "2021-06-15 11:50:16,506 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00277: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00297: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00317: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00337: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00357: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00377: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00417: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00437: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00457: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00477: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00497: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00517: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00537: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00557: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00557: early stopping\n",
      "2021-06-15 11:52:05,765 - modnet - INFO - loss: 0.2983\texp_gap_loss: 0.1698\tpbe_gap_loss: 0.1285\texp_gap_mae: 0.1698\tpbe_gap_mae: 0.1285\tlr: 0.0000\t\n",
      "2021-06-15 11:52:05,767 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00331: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00351: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00371: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00391: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00411: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00431: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00451: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00471: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00506: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00526: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00546: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00566: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00586: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00606: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00626: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00646: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00666: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00686: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00706: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00726: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00746: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00766: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00795: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00815: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00835: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00855: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00875: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00895: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "\n",
      "Epoch 00929: ReduceLROnPlateau reducing learning rate to 4.5474734072206875e-15.\n",
      "\n",
      "Epoch 00949: ReduceLROnPlateau reducing learning rate to 2.2737367036103438e-15.\n",
      "\n",
      "Epoch 00969: ReduceLROnPlateau reducing learning rate to 1.1368683518051719e-15.\n",
      "\n",
      "Epoch 00989: ReduceLROnPlateau reducing learning rate to 5.684341759025859e-16.\n",
      "2021-06-15 11:55:30,565 - modnet - INFO - loss: 0.3213\texp_gap_loss: 0.1846\tpbe_gap_loss: 0.1366\texp_gap_mae: 0.1846\tpbe_gap_mae: 0.1366\tlr: 0.0000\t\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.17it/s]\n",
      " 10%|█         | 1/10 [00:00<00:01,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           1.970\n",
      "  RMSE          29.821\n",
      "  MDAE          0.028\n",
      "  MARPD         138.551\n",
      "  R2            -308.647\n",
      "  Correlation   0.013\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.140\n",
      "  Mean-absolute Calibration Error       0.126\n",
      "  Miscalibration Area                   0.127\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.202\n",
      "     Group Size: 0.56 -- Calibration Error: 0.150\n",
      "     Group Size: 1.00 -- Calibration Error: 0.126\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.218\n",
      "     Group Size: 0.56 -- Calibration Error: 0.164\n",
      "     Group Size: 1.00 -- Calibration Error: 0.140\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   54.110\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   4170.839\n",
      "  CRPS                      1.332\n",
      "  Check Score               0.672\n",
      "  Interval Score            7.742\n",
      "MAE =\n",
      "0.3540961392502619\n",
      "uncertainty =\n",
      "0.12438768\n",
      "2021-06-15 11:55:41,093 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fbf2cff6ac0> object, created with modnet version 0.1.9\n",
      "2021-06-15 11:55:50,961 - modnet - INFO - Generation number 0\n",
      "2021-06-15 11:56:00,974 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:36<00:00, 17.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 12:25:37,611 - modnet - INFO - Loss per individual: ind 0: 0.688 \tind 1: 0.650 \tind 2: 0.613 \tind 3: 0.734 \tind 4: 0.570 \tind 5: 0.631 \tind 6: 0.633 \tind 7: 0.616 \tind 8: 1.106 \tind 9: 0.620 \tind 10: 0.622 \tind 11: 0.617 \tind 12: 0.673 \tind 13: 0.652 \tind 14: 0.594 \tind 15: 0.601 \tind 16: 0.685 \tind 17: 0.705 \tind 18: 0.696 \tind 19: 1.655 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 12:25:39,474 - modnet - INFO - Generation number 1\n",
      "2021-06-15 12:25:48,256 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [26:06<00:00, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 12:51:55,261 - modnet - INFO - Loss per individual: ind 0: 0.644 \tind 1: 0.627 \tind 2: 0.651 \tind 3: 0.658 \tind 4: 0.588 \tind 5: 0.625 \tind 6: 0.639 \tind 7: 0.696 \tind 8: 0.673 \tind 9: 0.700 \tind 10: 0.592 \tind 11: 0.618 \tind 12: 0.617 \tind 13: 0.672 \tind 14: 0.640 \tind 15: 0.609 \tind 16: 0.600 \tind 17: 0.592 \tind 18: 0.599 \tind 19: 0.616 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 12:51:58,556 - modnet - INFO - Generation number 2\n",
      "2021-06-15 12:52:12,255 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [28:56<00:00, 17.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 13:21:08,939 - modnet - INFO - Loss per individual: ind 0: 0.618 \tind 1: 0.722 \tind 2: 0.609 \tind 3: 0.626 \tind 4: 0.604 \tind 5: 0.678 \tind 6: 0.645 \tind 7: 0.580 \tind 8: 0.631 \tind 9: 0.698 \tind 10: 0.691 \tind 11: 0.605 \tind 12: 0.604 \tind 13: 0.610 \tind 14: 0.650 \tind 15: 0.661 \tind 16: 0.793 \tind 17: 0.656 \tind 18: 0.646 \tind 19: 0.641 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 13:21:12,335 - modnet - INFO - Generation number 3\n",
      "2021-06-15 13:21:27,530 - modnet - INFO - Multiprocessing on 10 cores. Total of 48 cores available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [28:47<00:00, 17.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 13:50:14,984 - modnet - INFO - Loss per individual: ind 0: 0.678 \tind 1: 4.338 \tind 2: 0.630 \tind 3: 0.621 \tind 4: 0.630 \tind 5: 0.640 \tind 6: 0.669 \tind 7: 0.578 \tind 8: 0.655 \tind 9: 0.641 \tind 10: 0.710 \tind 11: 0.605 \tind 12: 0.603 \tind 13: 0.647 \tind 14: 0.618 \tind 15: 0.620 \tind 16: 0.674 \tind 17: 0.625 \tind 18: 0.640 \tind 19: 0.631 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-15 13:50:18,301 - modnet - INFO - Early stopping: same best model for 4 consecutive generations\n",
      "mae_ph1\n",
      "0.38437087296975636\n",
      "2021-06-15 13:50:28,803 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 13:50:40,079 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00226: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00246: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00266: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00296: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00335: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00375: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00395: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00435: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00455: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00475: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00495: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00515: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00535: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00555: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00575: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00595: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00615: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00615: early stopping\n",
      "2021-06-15 13:52:29,451 - modnet - INFO - loss: 0.2235\texp_gap_loss: 0.1275\tpbe_gap_loss: 0.0960\texp_gap_mae: 0.1275\tpbe_gap_mae: 0.0960\tlr: 0.0000\t\n",
      "2021-06-15 13:52:29,456 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00288: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00308: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00328: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00348: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00368: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00388: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00408: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00433: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00453: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00473: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00493: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00513: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00533: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00553: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00573: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00593: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00615: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00635: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00655: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00675: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00695: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00715: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00747: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00767: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00787: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00807: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00827: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00847: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00867: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00887: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00907: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00927: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00947: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00967: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00987: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "2021-06-15 13:55:04,001 - modnet - INFO - loss: 0.2827\texp_gap_loss: 0.1661\tpbe_gap_loss: 0.1166\texp_gap_mae: 0.1661\tpbe_gap_mae: 0.1166\tlr: 0.0000\t\n",
      "2021-06-15 13:55:04,003 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00222: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00242: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00282: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00302: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00322: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00342: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00362: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00382: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00402: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00422: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00442: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00462: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00482: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00482: early stopping\n",
      "2021-06-15 13:56:28,125 - modnet - INFO - loss: 0.3162\texp_gap_loss: 0.1790\tpbe_gap_loss: 0.1372\texp_gap_mae: 0.1790\tpbe_gap_mae: 0.1372\tlr: 0.0000\t\n",
      "2021-06-15 13:56:28,128 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00206: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00307: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00327: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00352: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00372: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00392: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00412: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00432: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00452: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00472: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00492: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00512: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00532: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00552: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00572: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00592: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00612: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00632: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00632: early stopping\n",
      "2021-06-15 13:58:19,921 - modnet - INFO - loss: 0.2444\texp_gap_loss: 0.1457\tpbe_gap_loss: 0.0987\texp_gap_mae: 0.1457\tpbe_gap_mae: 0.0987\tlr: 0.0000\t\n",
      "2021-06-15 13:58:19,923 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00212: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00261: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00281: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00301: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00321: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00341: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00361: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00381: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00401: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00421: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00443: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00463: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00483: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00503: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00523: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00543: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00563: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00583: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00603: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00623: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00643: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00663: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00683: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00703: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00723: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00723: early stopping\n",
      "2021-06-15 14:00:24,589 - modnet - INFO - loss: 0.2595\texp_gap_loss: 0.1479\tpbe_gap_loss: 0.1117\texp_gap_mae: 0.1479\tpbe_gap_mae: 0.1117\tlr: 0.0000\t\n",
      "mae_ph2\n",
      "0.37432372367029204\n",
      "2021-06-15 14:00:27,401 - modnet - INFO - Generating bootstrap data...\n",
      "2021-06-15 14:00:31,632 - modnet - INFO - Bootstrap fitting model #1/5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00249: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00269: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00289: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00309: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00329: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00357: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00377: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00409: early stopping\n",
      "2021-06-15 14:01:39,487 - modnet - INFO - loss: 0.2860\texp_gap_loss: 0.1612\tpbe_gap_loss: 0.1248\texp_gap_mae: 0.1612\tpbe_gap_mae: 0.1248\tlr: 0.0000\t\n",
      "2021-06-15 14:01:39,489 - modnet - INFO - Bootstrap fitting model #2/5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00239: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00280: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00300: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00320: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00360: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00380: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00400: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00420: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00440: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00540: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00560: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00560: early stopping\n",
      "2021-06-15 14:03:17,630 - modnet - INFO - loss: 0.2999\texp_gap_loss: 0.1691\tpbe_gap_loss: 0.1307\texp_gap_mae: 0.1691\tpbe_gap_mae: 0.1307\tlr: 0.0000\t\n",
      "2021-06-15 14:03:17,636 - modnet - INFO - Bootstrap fitting model #3/5\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00261: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00281: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00301: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00321: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00367: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00387: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00407: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00442: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00462: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00482: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00502: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00522: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00542: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00562: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00582: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00602: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00622: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00642: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00662: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00682: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00703: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00723: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00743: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00763: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00783: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00803: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00823: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00843: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00863: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00883: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "\n",
      "Epoch 00903: ReduceLROnPlateau reducing learning rate to 9.094946814441375e-15.\n",
      "\n",
      "Epoch 00923: ReduceLROnPlateau reducing learning rate to 4.5474734072206875e-15.\n",
      "\n",
      "Epoch 00943: ReduceLROnPlateau reducing learning rate to 2.2737367036103438e-15.\n",
      "\n",
      "Epoch 00963: ReduceLROnPlateau reducing learning rate to 1.1368683518051719e-15.\n",
      "\n",
      "Epoch 00983: ReduceLROnPlateau reducing learning rate to 5.684341759025859e-16.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00983: early stopping\n",
      "2021-06-15 14:06:01,870 - modnet - INFO - loss: 0.2515\texp_gap_loss: 0.1464\tpbe_gap_loss: 0.1051\texp_gap_mae: 0.1464\tpbe_gap_mae: 0.1051\tlr: 0.0000\t\n",
      "2021-06-15 14:06:01,872 - modnet - INFO - Bootstrap fitting model #4/5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00240: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00260: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00280: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00300: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00320: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00360: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00380: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00400: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00420: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00440: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00520: early stopping\n",
      "2021-06-15 14:07:15,279 - modnet - INFO - loss: 0.2918\texp_gap_loss: 0.1905\tpbe_gap_loss: 0.1013\texp_gap_mae: 0.1905\tpbe_gap_mae: 0.1013\tlr: 0.0000\t\n",
      "2021-06-15 14:07:15,282 - modnet - INFO - Bootstrap fitting model #5/5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 00208: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 00268: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 00292: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 00312: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 00332: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "\n",
      "Epoch 00352: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "\n",
      "Epoch 00372: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "\n",
      "Epoch 00420: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "\n",
      "Epoch 00440: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "\n",
      "Epoch 00480: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 9.53674295089968e-09.\n",
      "\n",
      "Epoch 00540: ReduceLROnPlateau reducing learning rate to 4.76837147544984e-09.\n",
      "\n",
      "Epoch 00560: ReduceLROnPlateau reducing learning rate to 2.38418573772492e-09.\n",
      "\n",
      "Epoch 00580: ReduceLROnPlateau reducing learning rate to 1.19209286886246e-09.\n",
      "\n",
      "Epoch 00600: ReduceLROnPlateau reducing learning rate to 5.9604643443123e-10.\n",
      "\n",
      "Epoch 00631: ReduceLROnPlateau reducing learning rate to 2.98023217215615e-10.\n",
      "\n",
      "Epoch 00651: ReduceLROnPlateau reducing learning rate to 1.490116086078075e-10.\n",
      "\n",
      "Epoch 00671: ReduceLROnPlateau reducing learning rate to 7.450580430390374e-11.\n",
      "\n",
      "Epoch 00691: ReduceLROnPlateau reducing learning rate to 3.725290215195187e-11.\n",
      "\n",
      "Epoch 00711: ReduceLROnPlateau reducing learning rate to 1.8626451075975936e-11.\n",
      "\n",
      "Epoch 00731: ReduceLROnPlateau reducing learning rate to 9.313225537987968e-12.\n",
      "\n",
      "Epoch 00751: ReduceLROnPlateau reducing learning rate to 4.656612768993984e-12.\n",
      "\n",
      "Epoch 00771: ReduceLROnPlateau reducing learning rate to 2.328306384496992e-12.\n",
      "\n",
      "Epoch 00791: ReduceLROnPlateau reducing learning rate to 1.164153192248496e-12.\n",
      "\n",
      "Epoch 00811: ReduceLROnPlateau reducing learning rate to 5.82076596124248e-13.\n",
      "\n",
      "Epoch 00831: ReduceLROnPlateau reducing learning rate to 2.91038298062124e-13.\n",
      "\n",
      "Epoch 00851: ReduceLROnPlateau reducing learning rate to 1.45519149031062e-13.\n",
      "\n",
      "Epoch 00871: ReduceLROnPlateau reducing learning rate to 7.2759574515531e-14.\n",
      "\n",
      "Epoch 00891: ReduceLROnPlateau reducing learning rate to 3.63797872577655e-14.\n",
      "\n",
      "Epoch 00911: ReduceLROnPlateau reducing learning rate to 1.818989362888275e-14.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00911: early stopping\n",
      "2021-06-15 14:09:36,750 - modnet - INFO - loss: 0.2942\texp_gap_loss: 0.1662\tpbe_gap_loss: 0.1280\texp_gap_mae: 0.1662\tpbe_gap_mae: 0.1280\tlr: 0.0000\t\n",
      " (1/n) Calculating accuracy metrics\n",
      " (2/n) Calculating average calibration metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3/n) Calculating adversarial group calibration metrics\n",
      "  [1/2] for mean absolute calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.95it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] for root mean squared calibration error\n",
      "Measuring adversarial group calibration by spanning group size between 0.0 and 1.0, in 10 intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (4/n) Calculating sharpness metrics\n",
      " (n/n) Calculating proper scoring rule metrics\n",
      "**Finished Calculating All Metrics**\n",
      "\n",
      "\n",
      "===================== Accuracy Metrics =====================\n",
      "  MAE           0.367\n",
      "  RMSE          0.775\n",
      "  MDAE          0.029\n",
      "  MARPD         137.236\n",
      "  R2            0.802\n",
      "  Correlation   0.896\n",
      "=============== Average Calibration Metrics ================\n",
      "  Root-mean-squared Calibration Error   0.111\n",
      "  Mean-absolute Calibration Error       0.081\n",
      "  Miscalibration Area                   0.082\n",
      "========== Adversarial Group Calibration Metrics ===========\n",
      "  Mean-absolute Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.162\n",
      "     Group Size: 0.56 -- Calibration Error: 0.101\n",
      "     Group Size: 1.00 -- Calibration Error: 0.081\n",
      "  Root-mean-squared Adversarial Group Calibration Error\n",
      "     Group Size: 0.11 -- Calibration Error: 0.185\n",
      "     Group Size: 0.56 -- Calibration Error: 0.136\n",
      "     Group Size: 1.00 -- Calibration Error: 0.111\n",
      "==================== Sharpness Metrics =====================\n",
      "  Sharpness   0.216\n",
      "=================== Scoring Rule Metrics ===================\n",
      "  Negative-log-likelihood   135.623\n",
      "  CRPS                      0.312\n",
      "  Check Score               0.157\n",
      "  Interval Score            2.438\n",
      "MAE =\n",
      "0.36735822297663145\n",
      "uncertainty =\n",
      "0.122828774\n"
     ]
    }
   ],
   "source": [
    "import uncertainty_toolbox as uct\n",
    "\n",
    "k = 5\n",
    "random_state = 202010\n",
    "folds = MDKsplit(md_joint,n_splits=k,random_state=random_state)\n",
    "maes_ph1 = np.ones(5)\n",
    "maes_ph2 = np.ones(5)\n",
    "uncertainties = np.ones(5)\n",
    "metricss = []\n",
    "maes = np.ones(5)\n",
    "for i,f in enumerate(folds):\n",
    "    train = f[0]\n",
    "    test = f[1]\n",
    "    #train.feature_selection(n=-1, use_precomputed_cross_nmi=True)\n",
    "    fpath = 'train_{}_{}'.format(random_state,i+1)\n",
    "    if os.path.exists(fpath):\n",
    "        train = MODData.load(fpath)\n",
    "    else:\n",
    "        train.feature_selection(n=-1, use_precomputed_cross_nmi=True)\n",
    "        train.save(fpath)\n",
    "         \n",
    "    # assure no overlap\n",
    "    assert len(set(train.df_targets.index).intersection(set(test.df_targets.index))) == 0\n",
    "    \n",
    "    #phase 1\n",
    "    md = MD_append(train,[md_pbe,md_hse])\n",
    "    \n",
    "    ga = FitGenetic(md)\n",
    "    model = ga.run(size_pop=20, num_generations=10, n_jobs=10)\n",
    "    \n",
    "    pred, std = model.predict(test, return_unc=True)\n",
    "    true = test.df_targets\n",
    "    error = pred-true\n",
    "    error = error['exp_gap'].drop(pred.index[((pred['exp_gap']).abs()>20)]) # drop unrealistic values: happens extremely rarely\n",
    "    mae = np.abs(error.values).mean()\n",
    "    print('mae_ph1')\n",
    "    print(mae)\n",
    "    maes_ph1[i] = mae\n",
    "    \n",
    "    # phase2\n",
    "    md = MD_append(train,[md_hse])\n",
    "    rlr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=20, verbose=1, mode=\"auto\", min_delta=0)\n",
    "    es = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=300, verbose=1, mode=\"auto\", baseline=None,restore_best_weights=True)\n",
    "    model.fit(md,lr=0.01, epochs = 1000, batch_size = 64, loss='mae', callbacks=[rlr,es], verbose=0)\n",
    "    \n",
    "    pred, std = model.predict(test, return_unc=True)\n",
    "    true = test.df_targets\n",
    "    error = pred-true\n",
    "    error = error['exp_gap'].drop(pred.index[((pred['exp_gap']).abs()>20)]) # drop unrealistic values: happens extremely rarely\n",
    "    mae = np.abs(error.values).mean()\n",
    "    print('mae_ph2')\n",
    "    print(mae)\n",
    "    maes_ph2[i] = mae\n",
    "    \n",
    "        # phase2\n",
    "    rlr = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=20, verbose=1, mode=\"auto\", min_delta=0)\n",
    "    es = EarlyStopping(monitor=\"loss\", min_delta=0.001, patience=300, verbose=1, mode=\"auto\", baseline=None,restore_best_weights=True)\n",
    "    model.fit(train,lr=0.01, epochs = 1000, batch_size = 64, loss='mae', callbacks=[rlr,es], verbose=0)\n",
    "    \n",
    "    pred, std = model.predict(test, return_unc=True)\n",
    "    true = test.df_targets\n",
    "    metrics = uct.metrics.get_all_metrics(pred['exp_gap'].values, std['exp_gap'].values, true['exp_gap'].values)\n",
    "    error = pred-true\n",
    "    error = error['exp_gap'].drop(pred.index[((pred['exp_gap']).abs()>20)]) # drop unrealistic values: happens extremely rarely\n",
    "    std = std['exp_gap'].drop(std.index[((std['exp_gap']).abs()>20)]) # drop unrealistic values: happens extremely rarely\n",
    "    mae = np.abs(error.values).mean()\n",
    "    uncertainty = np.abs(std).mean()\n",
    "    print('MAE =')\n",
    "    print(mae)\n",
    "    print('uncertainty =')\n",
    "    print(uncertainty)\n",
    "    maes[i] = mae\n",
    "    uncertainties[i] = uncertainty\n",
    "    metricss.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.146241Z",
     "start_time": "2021-06-15T12:09:51.132949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3543464838457022"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes_ph1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.285190Z",
     "start_time": "2021-06-15T12:09:51.160006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.373707033235109"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes_ph2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.296881Z",
     "start_time": "2021-06-15T12:09:51.289064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43693347, 0.3993549 , 0.29553638, 0.35409614, 0.36735822])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.312579Z",
     "start_time": "2021-06-15T12:09:51.302319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37065582417176934"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.327221Z",
     "start_time": "2021-06-15T12:09:51.317562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16648124, 0.14064462, 0.10966733, 0.12438768, 0.12282877])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.342193Z",
     "start_time": "2021-06-15T12:09:51.330094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1328019306063652"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainties.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-15T12:09:51.430531Z",
     "start_time": "2021-06-15T12:09:51.347930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': {'mae': 0.43693347197395244,\n",
       "   'rmse': 0.9242280382492499,\n",
       "   'mdae': 0.07902669161558151,\n",
       "   'marpd': 136.24622227221295,\n",
       "   'r2': 0.6690793361767605,\n",
       "   'corr': 0.8327573954342087},\n",
       "  'avg_calibration': {'rms_cal': 0.18228671626396575,\n",
       "   'ma_cal': 0.1659237536656892,\n",
       "   'miscal_area': 0.16759975117746376},\n",
       "  'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.46470202, 0.24788729, 0.20871571, 0.2001991 , 0.18966261,\n",
       "           0.1909317 , 0.18514097, 0.17770543, 0.17479208, 0.16592375]),\n",
       "    'adv_group_cali_stderr': array([0.03359228, 0.03396076, 0.01718741, 0.00959553, 0.00930923,\n",
       "           0.01324697, 0.00904154, 0.00423822, 0.00285447, 0.        ])},\n",
       "   'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.51660031, 0.28328033, 0.22951233, 0.2308125 , 0.21430125,\n",
       "           0.20360226, 0.20120584, 0.19684831, 0.19276744, 0.18228672]),\n",
       "    'adv_group_cali_stderr': array([5.55257096e-02, 4.02094580e-02, 1.44228826e-02, 1.20610896e-02,\n",
       "           1.45922852e-02, 7.15045421e-03, 7.88871266e-03, 7.04722312e-03,\n",
       "           4.20220815e-03, 2.92569456e-17])}},\n",
       "  'sharpness': {'sharp': 0.30386806},\n",
       "  'scoring_rule': {'nll': 124.92639647155723,\n",
       "   'crps': 0.36757521533039084,\n",
       "   'check': 0.18490009088904055,\n",
       "   'interval': 2.779765792262708}},\n",
       " {'accuracy': {'mae': 0.3993549019144095,\n",
       "   'rmse': 0.8937184211840309,\n",
       "   'mdae': 0.06609265327453606,\n",
       "   'marpd': 135.3340582416085,\n",
       "   'r2': 0.6825782469634123,\n",
       "   'corr': 0.8314436104913245},\n",
       "  'avg_calibration': {'rms_cal': 0.24423098791913572,\n",
       "   'ma_cal': 0.2282111436950147,\n",
       "   'miscal_area': 0.23051630676264115},\n",
       "  'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.4785202 , 0.28302632, 0.26394737, 0.25029825, 0.25460526,\n",
       "           0.24626984, 0.24218062, 0.24027547, 0.23511221, 0.22821114]),\n",
       "    'adv_group_cali_stderr': array([1.49037743e-02, 2.31938351e-02, 1.15523352e-02, 9.73514728e-03,\n",
       "           1.73914432e-02, 5.84278889e-03, 5.63230977e-03, 3.80681594e-03,\n",
       "           2.11572922e-03, 2.92569456e-17])},\n",
       "   'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.50413626, 0.3123344 , 0.27515925, 0.2713796 , 0.26924861,\n",
       "           0.26114279, 0.2601263 , 0.25297263, 0.25147263, 0.24423099]),\n",
       "    'adv_group_cali_stderr': array([0.06948856, 0.01605257, 0.00888188, 0.01305988, 0.0070483 ,\n",
       "           0.00636345, 0.00765136, 0.00279053, 0.00160379, 0.        ])}},\n",
       "  'sharpness': {'sharp': 0.25494954},\n",
       "  'scoring_rule': {'nll': 133.15490873800263,\n",
       "   'crps': 0.3356404381941845,\n",
       "   'check': 0.16883485803289158,\n",
       "   'interval': 2.5393091103176326}},\n",
       " {'accuracy': {'mae': 8282.75697594734,\n",
       "   'rmse': 89922.0356894919,\n",
       "   'mdae': 0.018949825316667557,\n",
       "   'marpd': 138.44958342724362,\n",
       "   'r2': -3719372256.2864294,\n",
       "   'corr': -0.02036467124810946},\n",
       "  'avg_calibration': {'rms_cal': 0.1999775276290203,\n",
       "   'ma_cal': 0.1824633431085044,\n",
       "   'miscal_area': 0.1843064071803075},\n",
       "  'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.44932323, 0.23261749, 0.22018421, 0.21092105, 0.20225   ,\n",
       "           0.19757143, 0.19793392, 0.19701509, 0.19016502, 0.18246334]),\n",
       "    'adv_group_cali_stderr': array([6.41913880e-02, 2.12715513e-02, 1.30728538e-02, 1.04723478e-02,\n",
       "           5.35858303e-03, 5.80873508e-03, 6.50172277e-03, 5.70149843e-03,\n",
       "           3.44834919e-03, 2.92569456e-17])},\n",
       "   'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.47462271, 0.26833054, 0.24436646, 0.22422503, 0.22060729,\n",
       "           0.22018152, 0.21425098, 0.21258796, 0.2094913 , 0.19997753]),\n",
       "    'adv_group_cali_stderr': array([0.0785504 , 0.0169993 , 0.02238142, 0.00773296, 0.00676743,\n",
       "           0.00822233, 0.00581402, 0.00421938, 0.00303569, 0.        ])}},\n",
       "  'sharpness': {'sharp': 108986.29},\n",
       "  'scoring_rule': {'nll': 89.7147129112521,\n",
       "   'crps': 4927.674627357701,\n",
       "   'check': 2488.362107461417,\n",
       "   'interval': 22572.993802174246}},\n",
       " {'accuracy': {'mae': 1.96978118507232,\n",
       "   'rmse': 29.82099437484174,\n",
       "   'mdae': 0.02841886505484581,\n",
       "   'marpd': 138.55136504532973,\n",
       "   'r2': -308.6465395975443,\n",
       "   'corr': 0.013135881833875014},\n",
       "  'avg_calibration': {'rms_cal': 0.14000524160045807,\n",
       "   'ma_cal': 0.12561764705882358,\n",
       "   'miscal_area': 0.12688651218062982},\n",
       "  'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.4104697 , 0.20216241, 0.1795953 , 0.15623009, 0.15678641,\n",
       "           0.14986195, 0.14302385, 0.14012121, 0.13524172, 0.12561765]),\n",
       "    'adv_group_cali_stderr': array([6.66790794e-02, 2.35264787e-02, 1.78350501e-02, 1.12380631e-02,\n",
       "           1.14998664e-02, 9.15976845e-03, 4.42518427e-03, 4.98396188e-03,\n",
       "           4.68025140e-03, 2.92569456e-17])},\n",
       "   'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.51901042, 0.21821216, 0.18137172, 0.17408395, 0.17297672,\n",
       "           0.16373   , 0.16017286, 0.15423523, 0.15051037, 0.14000524]),\n",
       "    'adv_group_cali_stderr': array([0.07412842, 0.02611325, 0.02011092, 0.01401387, 0.01342722,\n",
       "           0.01009371, 0.00643055, 0.00414294, 0.00407233, 0.        ])}},\n",
       "  'sharpness': {'sharp': 54.110416},\n",
       "  'scoring_rule': {'nll': 4170.839259562431,\n",
       "   'crps': 1.3319702156729978,\n",
       "   'check': 0.6719029156683627,\n",
       "   'interval': 7.742247070501845}},\n",
       " {'accuracy': {'mae': 0.36735822297663145,\n",
       "   'rmse': 0.7750858284289497,\n",
       "   'mdae': 0.02868802286684513,\n",
       "   'marpd': 137.23643511059907,\n",
       "   'r2': 0.8017007439670774,\n",
       "   'corr': 0.8959588335808385},\n",
       "  'avg_calibration': {'rms_cal': 0.11147333829530551,\n",
       "   'ma_cal': 0.08121449792038032,\n",
       "   'miscal_area': 0.08201327454622209},\n",
       "  'adv_group_calibration': {'ma_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.44075758, 0.16156911, 0.13714926, 0.11727657, 0.1134318 ,\n",
       "           0.10127177, 0.10284853, 0.09581313, 0.09067787, 0.0812145 ]),\n",
       "    'adv_group_cali_stderr': array([6.18412431e-02, 3.98909743e-02, 1.15314263e-02, 9.57715597e-03,\n",
       "           1.06178554e-02, 7.12582843e-03, 6.20407977e-03, 4.37158176e-03,\n",
       "           3.72679322e-03, 1.46284728e-17])},\n",
       "   'rms_adv_group_cal': {'group_sizes': array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "           0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]),\n",
       "    'adv_group_cali_mean': array([0.47982295, 0.18469721, 0.17451837, 0.15081618, 0.13782722,\n",
       "           0.13631582, 0.12830595, 0.12544535, 0.12129908, 0.11147334]),\n",
       "    'adv_group_cali_stderr': array([0.10871873, 0.03289562, 0.02579865, 0.01531294, 0.00988877,\n",
       "           0.00739114, 0.00346804, 0.00510547, 0.00231872, 0.        ])}},\n",
       "  'sharpness': {'sharp': 0.21590033},\n",
       "  'scoring_rule': {'nll': 135.62327035331708,\n",
       "   'crps': 0.31242154144481504,\n",
       "   'check': 0.15706936243921318,\n",
       "   'interval': 2.43796663736039}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative percentage change of the error compared to MODNet_exp_GA reference: -7%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modnet]",
   "language": "python",
   "name": "conda-env-modnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
